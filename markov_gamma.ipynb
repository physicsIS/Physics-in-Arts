{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/physicsIS/Physics-in-Arts/blob/Poetry/markov_gamma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhqz96O3a8c3"
      },
      "source": [
        "### Generative Poetry\n",
        "\n",
        "In this notebook, we adapt the algorithm proposed in *Astrophysical Narratives: Poetic Representations of Gamma-Ray Emission via Markov Chains*. This is an algorithm for text generation that adapts the classic Markov chain model to a small corpus, using the distribution of astrophysical gamma-ray sources detected by the FermiLAT satellite as the state matrix.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9Z8DnL_PcqA",
        "outputId": "ffdd2b63-9806-4f1e-fccf-e8b73adc84aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.67.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (0.45.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.8.1\n",
            "Collecting es-dep-news-trf==3.7.2\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_dep_news_trf-3.7.2/es_dep_news_trf-3.7.2-py3-none-any.whl (407.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.8/407.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-dep-news-trf==3.7.2) (3.7.5)\n",
            "Collecting spacy-curated-transformers<0.3.0,>=0.2.0 (from es-dep-news-trf==3.7.2)\n",
            "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.26.4)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2)\n",
            "  Downloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (2.5.1+cu121)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.10/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->es-dep-news-trf==3.7.2) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-dep-news-trf==3.7.2) (0.1.2)\n",
            "Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (731 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, es-dep-news-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 es-dep-news-trf-3.7.2 spacy-curated-transformers-0.2.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_dep_news_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2024.12.14)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.8.2\n",
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (5.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.32.3)\n",
            "Collecting libretranslatepy==2.1.1 (from translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2024.12.14)\n",
            "Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  #------------------------------------------------ Connect Google Drive with Colab to read and store files\n",
        "\n",
        "# Install necessary dependencies\n",
        "!pip install opencv-python-headless numpy language_tool_python spacy nltk\n",
        "!python -m spacy download es_dep_news_trf  #------------------------------------ Download the model 'es_dep_news_trf'\n",
        "!pip install gpt4all\n",
        "!pip install translate\n",
        "!pip install deep-translator\n",
        "\n",
        "import cv2  #------------------------------------------------------------------- OpenCV for image processing\n",
        "from google.colab.patches import cv2_imshow  #---------------------------------- Display images in Google Colab\n",
        "import numpy as np  #----------------------------------------------------------- NumPy for numerical operations\n",
        "import random  #---------------------------------------------------------------- Random number generation\n",
        "import language_tool_python  #-------------------------------------------------- Grammar checking tool\n",
        "import spacy  #----------------------------------------------------------------- Natural language processing\n",
        "import nltk  #------------------------------------------------------------------ NLTK for natural language processing\n",
        "from nltk.corpus import stopwords, wordnet  #----------------------------------- Stopwords and WordNet for natural language processing\n",
        "import re  #-------------------------------------------------------------------- Regular expressions for text processing\n",
        "from gpt4all import GPT4All  #-------------------------------------------------- GPT-4 All language model for text correction\n",
        "from translate import Translator  #--------------------------------------------- Package for text translation\n",
        "from deep_translator import GoogleTranslator  #--------------------------------- Package for text translation\n",
        "from IPython.display import HTML  #--------------------------------------------- Display HTML content in Colab\n",
        "import math  #------------------------------------------------------------------ Math operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz0GwWJoQCfd"
      },
      "outputs": [],
      "source": [
        "# Load the GPT-4 All model\n",
        "model = GPT4All(model_name=\"Hermes-2-Pro-Llama-3-8B-Q5_K_M.gguf\", allow_download=True)\n",
        "# If the model is not already downloaded locally (or in Drive), set \"allow_download=True\" to download it.\n",
        "# You can choose the model according to your preference from GPT4ALL or Hugging Face, just download the .gguf file\n",
        "\n",
        "# Load the spaCy model for text analysis\n",
        "nlp = spacy.load(\"es_dep_news_trf\")  #------------------------------------------ Using the transformer model specific to Spanish\n",
        "nlp.max_length = 2000000  #----------------------------------------------------- Increase the processing limit to 2 million characters (adjust as necessary)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KnrPdz-RV-h"
      },
      "outputs": [],
      "source": [
        "# Tokenize and lemmatize the corpus of poems using spaCy\n",
        "def load_poem(path):  #--------------------------------------------------------- Function for tokenization and lemmatization while preserving punctuation\n",
        "    \"\"\"\"\n",
        "    The tokenization function uses transformers,\n",
        "    which can slow down the code depending on\n",
        "    the size of the corpus being loaded.\"\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    doc = nlp(text)  #---------------------------------------------------------- Process the text with spaCy\n",
        "    # Process tokens while preserving punctuation and only removing spaces\n",
        "    processed = [\n",
        "        token.text if token.is_punct else token.lemma_  #----------------------- Keep original text if punctuation\n",
        "        for token in doc\n",
        "        if not token.is_space  #------------------------------------------------ Only removes spaces\n",
        "    ]\n",
        "    return processed\n",
        "\n",
        "# def load_poem(path):  #------------------------------------------------------- Function for tokenization and lemmatization ignoring punctuation\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         text = f.read()\n",
        "#     doc = nlp(text)  #-------------------------------------------------------- Process the text with spaCy to obtain tokens and lemmas\n",
        "#     lemmatized = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "#     return lemmatized\n",
        "\n",
        "# Only tokenize the corpus\n",
        "def load_tokenized_poem(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    doc = nlp(text)  #---------------------------------------------------------- Process the text with spaCy to obtain tokens\n",
        "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
        "    return tokens\n",
        "\n",
        "# Function to get synonyms [Spanish] using WordNet\n",
        "def get_synonym_es(word):\n",
        "    try:\n",
        "        # Translate the word to English\n",
        "        translator_es_en = GoogleTranslator(source='es', target='en')\n",
        "        word_en = translator_es_en.translate(word).lower()\n",
        "\n",
        "        # Get synonyms in English\n",
        "        synonyms = set()\n",
        "        for synset in wordnet.synsets(word_en):\n",
        "            for lemma in synset.lemmas():\n",
        "                # Avoid adding the same word as a synonym\n",
        "                if lemma.name().lower() != word_en:\n",
        "                    synonyms.add(lemma.name())\n",
        "\n",
        "        # If no synonyms are found, return the original word\n",
        "        if not synonyms:\n",
        "            print(f\"No synonyms found for '{word}'\")\n",
        "            return word\n",
        "\n",
        "        # Select a random synonym\n",
        "        synonym_en = random.choice(list(synonyms))\n",
        "\n",
        "        # Replace underscores with spaces\n",
        "        synonym_en = synonym_en.replace('_', ' ')\n",
        "\n",
        "        # Translate the synonym back to Spanish\n",
        "        translator_en_es = GoogleTranslator(source='en', target='es')\n",
        "        synonym_es = translator_en_es.translate(synonym_en)\n",
        "        print(synonyms)\n",
        "        return synonym_es\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the word: {str(e)}\")\n",
        "        return word\n",
        "\n",
        "# Function to get synonyms [English] using WordNet\n",
        "# def get_synonym_en(word):\n",
        "#     try:\n",
        "#         # Get synonyms in English\n",
        "#         synonyms = set()\n",
        "#         for synset in wordnet.synsets(word_en):\n",
        "#             for lemma in synset.lemmas():\n",
        "#                 # Avoid adding the same word as a synonym\n",
        "#                 if lemma.name().lower() != word_en:\n",
        "#                     synonyms.add(lemma.name())\n",
        "\n",
        "#         # If no synonyms are found, return the original word\n",
        "#         if not synonyms:\n",
        "#             print(f\"No synonyms found for '{word}'\")\n",
        "#             return word\n",
        "\n",
        "#         # Select a random synonym\n",
        "#         synonym_en = random.choice(list(synonyms))\n",
        "\n",
        "#         # Replace underscores with spaces\n",
        "#         synonym_en = synonym_en.replace('_', ' ')\n",
        "\n",
        "#         # Translate the synonym back to Spanish\n",
        "#         print(synonyms)\n",
        "#         return synonym_en\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing the word: {str(e)}\")\n",
        "#         return word\n",
        "\n",
        "\n",
        "# Function to correct grammar using LanguageTool\n",
        "def grammar_corrector(text):\n",
        "    tool = language_tool_python.LanguageTool('es')\n",
        "    return tool.correct(text)\n",
        "\n",
        "# Function to correct the poem using a language model via GPT-4 All\n",
        "def correct_poem(poem): #------------------------------------------------------- Change the promtp to taste depending on the language\n",
        "    correction_prompt = f\"Corrige el siguiente poema (verso) en español, conjugando adecuadamente las palabras lematizadas para darles un sentido coherente y poético, sin alejarte del significado original. Organiza el resultado de manera que fluya como un poema natural, con la estructura y la puntuación adecuada. Devuelve solo el texto corregido dentro de corchetes, sin incluir nada más:\\n\\n{poem}\"\n",
        "\n",
        "    with model.chat_session() as chat:  #--------------------------------------- Initialize the model in chat mode for better response\n",
        "        corrected_poem = chat.generate(correction_prompt)\n",
        "    return corrected_poem.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdzmMyR5SXaO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In this section, the corpus is loaded and processed.\n",
        "This can be any plain text file. The longer the corpus,\n",
        "the better the results the algorithm will produce.\n",
        "Users are encouraged to utilize not only their own poems\n",
        "or texts but also poems, books, and songs they enjoy.\n",
        "Humans learn by imitation, especially in artistic matters.\n",
        "We are all influenced by external factors, references,\n",
        "and more.\n",
        "\"\"\"\n",
        "# Load image from a specified path\n",
        "image_0 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image_1 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799_2.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image_2 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799_3.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image_3 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799_4.png\", cv2.IMREAD_GRAYSCALE)\n",
        "cv2_imshow(image_0)  #------------------------------------------------------------ Display the image in a window\n",
        "cv2_imshow(image_1)  #------------------------------------------------------------ Display the image in a window\n",
        "cv2_imshow(image_2)  #------------------------------------------------------------ Display the image in a window\n",
        "cv2_imshow(image_3)  #------------------------------------------------------------ Display the image in a window\n",
        "print(\"Image loaded.\")\n",
        "\n",
        "# Load text files for poems from a specified path\n",
        "poems0 = load_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/Primer_internamiento.txt\")\n",
        "poems1 = load_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/relatos.txt\")\n",
        "poems2 = load_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/cartas_.txt\")\n",
        "poems3 = load_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/Garmar_antiguo.txt\")\n",
        "poems4 = load_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/externos.txt\")\n",
        "poems5 = load_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/letras.txt\")\n",
        "# etc. add as many files as you need.\n",
        "\n",
        "# Tokenize text files for poems\n",
        "poems0_tokenized = load_tokenized_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/Primer_internamiento.txt\")\n",
        "poems1_tokenized = load_tokenized_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/relatos.txt\")\n",
        "poems2_tokenized = load_tokenized_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/cartas_.txt\")\n",
        "poems3_tokenized = load_tokenized_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/Garmar_antiguo.txt\")\n",
        "poems4_tokenized = load_tokenized_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/externos.txt\")\n",
        "poems5_tokenized = load_tokenized_poem(\"/content/drive/MyDrive/Stars_Poems/Corpus/letras.txt\")\n",
        "#same here, add as many files as you need them :)\n",
        "\n",
        "print(\"Poem files loaded.\")\n",
        "\n",
        "# Combine all lemmatized poems into a single list\n",
        "# Here you can select if the corpus is lemmatized or tokenized by changing the string \"poems\"\n",
        "#poems = poems0 + poems1 + poems2 + poems3 + poems4 + poems5\n",
        "poems = poems0_tokenized + poems1_tokenized + poems2_tokenized + poems3_tokenized + poems4_tokenized + poems5_tokenized\n",
        "print(\"Lemmatized poems unified.\")\n",
        "\n",
        "# Save the result of tokenization and lemmatization to a file with one word per line\n",
        "#This is useful for compare the correct function of the tokenization and lemmatization funtions\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/lemmatized_poems.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write('\\n'.join(poems))  #--------------------------------------------- Each word on a new line\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/tokenized_poems.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write('\\n'.join(poems_tokenized))  #----------------------------------- Each word on a new line\n",
        "\n",
        "print(\"File with lemmatization generated (one word per line).\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/Corpus.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(' '.join(poems))\n",
        "\n",
        "print(\"File with the final corpus.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcb2HhclTf1K"
      },
      "outputs": [],
      "source": [
        "# Create a Markov chain dictionary avoiding duplicates but maintaining order\n",
        "chain = {}\n",
        "for index, word in enumerate(poems[1:], 1):\n",
        "    key = poems[index - 1]\n",
        "    if key not in chain:\n",
        "        chain[key] = []\n",
        "    if word not in chain[key]:\n",
        "        chain[key].append(word)\n",
        "\n",
        "print(\"Markov chain dictionary created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vHHNPOoT-lV"
      },
      "outputs": [],
      "source": [
        "# Generate a poem\n",
        "word1 = random.choice(list(chain.keys()))\n",
        "message = word1.capitalize()\n",
        "count = 7  #------------------------------------------------------------------- Total number of words in the poem\n",
        "words_per_line = 10  #---------------------------------------------------------- Number of words per line\n",
        "\n",
        "while len(message.split(' ')) < count:\n",
        "\n",
        "    \"\"\"This step is necessary to avoid stationary states;\n",
        "    it is the same gamma-ray distribution with different\n",
        "    color scale treatments and inversions.\"\"\"\n",
        "    selc = random.randint(0, 3)\n",
        "    print()\n",
        "    if selc == 0:\n",
        "        print(\"imagen usada: 1\")\n",
        "        image = image_0\n",
        "    elif selc == 1:\n",
        "        print(\"imagen usada: 2\")\n",
        "        image = image_1\n",
        "    elif selc == 2:\n",
        "        print(\"imagen usada: 3\")\n",
        "        image = image_2\n",
        "    elif selc == 3:\n",
        "        print(\"imagen usada: 4\")\n",
        "        image = image_3\n",
        "\n",
        "\n",
        "    wp = np.ones(len(chain[word1])) / len(chain[word1])\n",
        "    n = len(wp)\n",
        "\n",
        "    # Create Markov matrix from the image\n",
        "    if len(wp) <= image.shape[0]:\n",
        "        markov_n = np.zeros((n, n))\n",
        "        num_sub = image.shape[0] // n\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                sub_mat = image[i*num_sub:(i+1)*num_sub, j*num_sub:(j+1)*num_sub]\n",
        "                markov_n[i, j] = np.mean(sub_mat)\n",
        "    else:\n",
        "        markov_n = cv2.resize(image, (n, n))\n",
        "\n",
        "    # Normalize rows\n",
        "    markov_n = markov_n.astype(np.float64)\n",
        "    markov_n /= markov_n.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Choose the next word based on the matrix\n",
        "    next_w = np.dot(wp, markov_n)\n",
        "    if len(next_w) > 1:\n",
        "        selc_w = np.argmax(next_w)\n",
        "        word2 = chain[word1][selc_w]\n",
        "    else:\n",
        "        word2 = random.choice(list(chain.keys()))\n",
        "\n",
        "\n",
        "    print(next_w[np.argmax(next_w)])\n",
        "    print(\"Current word: \" + word1)\n",
        "    print(\"Number of options: \" + str(len(chain[word1])))\n",
        "    if len(chain[word1]) <= 1:\n",
        "        print(\"No options\")\n",
        "    print(chain[word1])\n",
        "    print(\"Next word: \" + word2)\n",
        "\n",
        "    # Get synonyms if needed\n",
        "    if np.amax(next_w) > 0.8:\n",
        "        word2_2 = get_synonym_es(word2)\n",
        "        #word2_2 = get_synonym_en(word2) #-------------------------------------- If you are using a corpus in English, use this function.\n",
        "                                                                                #Uncomment it in the function declaration section.\n",
        "        #print(\"Condition: yes\")\n",
        "        print(\"Synonym used: \" + word2_2)\n",
        "    else:\n",
        "        word2_2 = word2\n",
        "        #print(\"Condition: no\")\n",
        "\n",
        "    word1 = word2\n",
        "    message += ' ' + word2_2\n",
        "\n",
        "    # Add a line break after a certain number of words\n",
        "    if len(message.split(' ')) % words_per_line == 0:\n",
        "        message = message.rstrip() + '\\n'\n",
        "\n",
        "print(\"Generated poem:\")\n",
        "print(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OmfcShSmUITp"
      },
      "outputs": [],
      "source": [
        "# Apply final grammar correction\n",
        "corrected_message = grammar_corrector(message)\n",
        "\n",
        "# Correct the poem using GPT-4 All\n",
        "output_data = correct_poem(corrected_message)\n",
        "\n",
        "final_message = f\"{message}\\n\\nCorrected version:\\n{output_data}\"\n",
        "\n",
        "# Save results to a specified directory\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/Complete_Results.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(final_message.strip())\n",
        "\n",
        "# Read the file, ignoring any invalid characters\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/Complete_Results.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as output:\n",
        "    print(output.read())\n",
        "\n",
        "print(\"Process completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcAi22C3fQgM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Application of Concrete Poetry\n",
        "\n",
        "**[Optional: The previous result is a fully functional poem, which is the objective of the algorithm. From this point onward, only visual formatting will be applied.]**\n",
        "\n",
        "Depending on the energy value of the last word of the poem, a corresponding function will be selected to create a specific visual shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ1qSWvQjQeh"
      },
      "outputs": [],
      "source": [
        "# Convert text to HTML with random words in italics\n",
        "def text_to_html_random_italics(text, num_italics=1):\n",
        "    words = text.split()\n",
        "    num_italics = min(num_italics, len(words))\n",
        "    random_indices = random.sample(range(len(words)), num_italics)\n",
        "    for idx in random_indices:\n",
        "        words[idx] = f\"<i>{words[idx]}</i>\"\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Shows the text formatted in Colab\n",
        "def display_formatted_text(text):\n",
        "    display(HTML(text))\n",
        "\n",
        "# 1. Diamond\n",
        "def create_diamond(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    max_width = len(words_list) // 2 + 1\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(1, max_width + 1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "    for i in range(max_width - 1, 0, -1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 2. Triangle\n",
        "def create_triangle(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(1, len(words_list) + 1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 3. Inverted triangle\n",
        "def create_reverse_triangle(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(len(words_list), 0, -1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 4. Hourglass\n",
        "def create_hourglass(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(len(words_list), 0, -1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "    for i in range(2, len(words_list) + 1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 5. Spiral\n",
        "def create_spiral(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    size = math.ceil(math.sqrt(len(words_list)))\n",
        "    matrix = [['' for _ in range(size)] for _ in range(size)]\n",
        "\n",
        "    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "    x, y = 0, 0\n",
        "    dir_idx = 0\n",
        "    word_idx = 0\n",
        "\n",
        "    while word_idx < len(words_list):\n",
        "        if (0 <= x < size and 0 <= y < size and matrix[x][y] == ''):\n",
        "            matrix[x][y] = words_list[word_idx]\n",
        "            word_idx += 1\n",
        "\n",
        "        next_x = x + directions[dir_idx][0]\n",
        "        next_y = y + directions[dir_idx][1]\n",
        "\n",
        "        if (next_x < 0 or next_x >= size or next_y < 0 or next_y >= size or matrix[next_x][next_y] != ''):\n",
        "            dir_idx = (dir_idx + 1) % 4\n",
        "\n",
        "        x += directions[dir_idx][0]\n",
        "        y += directions[dir_idx][1]\n",
        "\n",
        "    html_output = []\n",
        "    for row in matrix:\n",
        "        line = ' '.join(word for word in row if word)\n",
        "        if line.strip():\n",
        "            html_output.append(line)\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 6. Zigzag\n",
        "def create_zigzag(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces = '&nbsp;' * (4 * (i % 6))\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 7. Circle aprox\n",
        "def create_circle(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    radius = len(words_list) // 4\n",
        "    for i, word in enumerate(words_list):\n",
        "        angle = (i * 2 * math.pi) / len(words_list)\n",
        "        spaces = '&nbsp;' * int(radius + radius * math.cos(angle))\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 8. Wave\n",
        "def create_wave(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces = '&nbsp;' * (10 + int(5 * math.sin(i * 0.5)))\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 9. Cross\n",
        "def create_cross(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    middle = len(words_list) // 2\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(len(words_list)):\n",
        "        if i == middle:\n",
        "            html_output.append(' '.join(words_list))\n",
        "        else:\n",
        "            spaces = '&nbsp;' * (len(words_list[middle]) * 2)\n",
        "            html_output.append(f'{spaces}{words_list[i]}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace; text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 10. Cascade\n",
        "def create_cascade(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces = '&nbsp;' * (i * 4)\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 11. Square\n",
        "def create_square(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    size = math.ceil(math.sqrt(len(words_list)))\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(size):\n",
        "        line_words = words_list[i*size:min((i+1)*size, len(words_list))]\n",
        "        line = ' '.join(line_words)\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 12. Random Scatter\n",
        "def create_random_scatter(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for word in words_list:\n",
        "        spaces = '&nbsp;' * random.randint(0, 40)\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 13. Arrow\n",
        "def create_arrow(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    middle = len(words_list) // 2\n",
        "    html_output = []\n",
        "\n",
        "    # Punta de la flecha\n",
        "    for i in range(middle):\n",
        "        line = ' '.join(words_list[i:i+1])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    # Línea central\n",
        "    html_output.append(' '.join(words_list[middle:]).center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 14. Pyramid\n",
        "def create_pyramid(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "    step_size = 2\n",
        "\n",
        "    for i in range(0, len(words_list), step_size):\n",
        "        line = ' '.join(words_list[i:i+step_size])\n",
        "        spaces = '&nbsp;' * (i * 2)\n",
        "        html_output.append(f'{spaces}{line}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 15. Helix\n",
        "def create_helix(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "    amplitude = 20\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces_left = '&nbsp;' * int(amplitude + amplitude * math.sin(i * 0.5))\n",
        "        spaces_right = '&nbsp;' * int(amplitude + amplitude * math.cos(i * 0.5))\n",
        "        html_output.append(f'{spaces_left}{word}{spaces_right}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "def create_concrete_poetry(text, form, max_value = 15):\n",
        "#form (float or str): Decimal number (0 to max_value) or name of the desired form\n",
        "#max_value (float): Maximum value for the input range (default: 15.0)\n",
        "    available_functions = [\n",
        "        create_diamond,\n",
        "        create_triangle,\n",
        "        create_reverse_triangle,\n",
        "        create_hourglass,\n",
        "        create_spiral,\n",
        "        create_zigzag,\n",
        "        create_circle,\n",
        "        create_wave,\n",
        "        create_cross,\n",
        "        create_cascade,\n",
        "        create_square,\n",
        "        create_random_scatter,\n",
        "        create_arrow,\n",
        "        create_pyramid,\n",
        "        create_helix\n",
        "    ]\n",
        "\n",
        "    #Dictionary for shape names\n",
        "    named_functions = {\n",
        "        'diamond': create_diamond,\n",
        "        'triangle': create_triangle,\n",
        "        'reverse_triangle': create_reverse_triangle,\n",
        "        'hourglass': create_hourglass,\n",
        "        'spiral': create_spiral,\n",
        "        'zigzag': create_zigzag,\n",
        "        'circle': create_circle,\n",
        "        'wave': create_wave,\n",
        "        'cross': create_cross,\n",
        "        'cascade': create_cascade,\n",
        "        'square': create_square,\n",
        "        'random_scatter': create_random_scatter,\n",
        "        'arrow': create_arrow,\n",
        "        'pyramid': create_pyramid,\n",
        "        'helix': create_helix\n",
        "    }\n",
        "\n",
        "    if isinstance(form, (int, float)):\n",
        "        if form < 0 or form > max_value:\n",
        "            form = random.random() * max_value\n",
        "            print(f\"Value out of range. Using random value: {form:.3f}\")\n",
        "        # Map the decimal value to the function index\n",
        "        normalized_value = form / max_value  # Normalize the value between 0 and 1\n",
        "        index = int(normalized_value * len(available_functions))\n",
        "        # Adjust for the case where form == max_value\n",
        "        if index == len(available_functions):\n",
        "            index = len(available_functions) - 1\n",
        "\n",
        "        selected_function = available_functions[index]\n",
        "        print(f\"Value {form:.3f} mapped to shape #{index + 1}\")\n",
        "    elif isinstance(form, str):\n",
        "        if form.lower() not in named_functions:\n",
        "            form = random.random() * max_value\n",
        "            print(f\"Invalid shape name. Using random value: {form:.3f}\")\n",
        "            normalized_value = form / max_value\n",
        "            index = int(normalized_value * len(available_functions))\n",
        "            selected_function = available_functions[index]\n",
        "        else:\n",
        "            selected_function = named_functions[form.lower()]\n",
        "    else:\n",
        "        raise ValueError(\"The 'form' parameter must be a number or a string\")\n",
        "\n",
        "    selected_function(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIA9VUhOpFRH"
      },
      "outputs": [],
      "source": [
        "num = ((np.argmax(next_w)-np.amin(markov_n))*15)/(np.argmax(markov_n) - np.amin(markov_n))\n",
        "print(num)\n",
        "create_concrete_poetry(output_data, form=num, max_value=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlwHwVSKw3Mr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VcAi22C3fQgM"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyP0vNEQd/T819uQnTPHx5lu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}