{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/physicsIS/Physics-in-Arts/blob/main/gamma_ray_markov_chain_poems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhqz96O3a8c3"
      },
      "source": [
        "### Generative Poetry\n",
        "\n",
        "In this notebook, we adapt the algorithm proposed in *Astrophysical Narratives: Poetic Representations of Gamma-Ray Emission via Markov Chains*. This is an algorithm for text generation that adapts the classic Markov chain model to a small corpus, using the distribution of astrophysical gamma-ray sources detected by the FermiLAT satellite as the state matrix.\n",
        "\n",
        "A long time ago, I stopped caring whether the world knows my mind, my soul, and my heart. So here, in the corpus (in the folder), are the original writings from my hospitalization and the corresponding letters. Please be discreet; what you read is the work of a broken soul that has tried to leave this world but has only managed to give a voice to the stars. These are the visible fragments of a psychiatric patient’s mind, the tears spilled onto paper.\n",
        "\n",
        "Additionally, there are included writings and songs I’ve cherished throughout my life. Alejandra Pizarnik, Love of Lesbian, Carlos Sadness, among others, accompany my writings.\n",
        "\n",
        "Keep in mind that most of the results will not make sense, run the poem generator several times until you find a verse that fits to you.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9Z8DnL_PcqA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  #------------------------------------------------ Connect Google Drive with Colab to read and store files\n",
        "                                                                                # or you can add your files directly to the colab\n",
        "\n",
        "# Install necessary dependencies\n",
        "!pip install opencv-python-headless numpy language_tool_python spacy nltk\n",
        "!python -m spacy download es_dep_news_trf  #------------------------------------ Download the model 'es_dep_news_trf'\n",
        "!pip install gpt4all\n",
        "!pip install translate\n",
        "!pip install deep-translator\n",
        "\n",
        "import cv2  #------------------------------------------------------------------- OpenCV for image processing\n",
        "from google.colab.patches import cv2_imshow  #---------------------------------- Display images in Google Colab\n",
        "import numpy as np  #----------------------------------------------------------- NumPy for numerical operations\n",
        "import random  #---------------------------------------------------------------- Random number generation\n",
        "import language_tool_python  #-------------------------------------------------- Grammar checking tool\n",
        "import spacy  #----------------------------------------------------------------- Natural language processing\n",
        "import nltk  #------------------------------------------------------------------ NLTK for natural language processing\n",
        "from nltk.corpus import stopwords, wordnet  #----------------------------------- Stopwords and WordNet for natural language processing\n",
        "import re  #-------------------------------------------------------------------- Regular expressions for text processing\n",
        "from gpt4all import GPT4All  #-------------------------------------------------- GPT-4 All language model for text correction\n",
        "from translate import Translator  #--------------------------------------------- Package for text translation\n",
        "from deep_translator import GoogleTranslator  #--------------------------------- Package for text translation\n",
        "from IPython.display import HTML  #--------------------------------------------- Display HTML content in Colab\n",
        "import math  #------------------------------------------------------------------ Math operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz0GwWJoQCfd"
      },
      "outputs": [],
      "source": [
        "# Load the GPT-4 All model\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
        "# If the model is not already downloaded locally (or in Drive), set \"allow_download=True\" to download it.\n",
        "# You can choose the model according to your preference from GPT4ALL or Hugging Face, just download the .gguf file\n",
        "\n",
        "# Load the spaCy model for text analysis\n",
        "nlp = spacy.load(\"es_dep_news_trf\")  #------------------------------------------ Using the transformer model specific to Spanish\n",
        "nlp.max_length = 2000000  #----------------------------------------------------- Increase the processing limit to 2 million characters (adjust as necessary)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KnrPdz-RV-h"
      },
      "outputs": [],
      "source": [
        "# Tokenize and lemmatize the corpus of poems using spaCy\n",
        "def load_poem(path):  #--------------------------------------------------------- Function for tokenization and lemmatization while preserving punctuation\n",
        "    \"\"\"\"\n",
        "    The tokenization function uses transformers,\n",
        "    which can slow down the code depending on\n",
        "    the size of the corpus being loaded.\"\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    doc = nlp(text)  #---------------------------------------------------------- Process the text with spaCy\n",
        "    # Process tokens while preserving punctuation and only removing spaces\n",
        "    processed = [\n",
        "        token.text if token.is_punct else token.lemma_  #----------------------- Keep original text if punctuation\n",
        "        for token in doc\n",
        "        if not token.is_space  #------------------------------------------------ Only removes spaces\n",
        "    ]\n",
        "    return processed\n",
        "\n",
        "# def load_poem(path):  #------------------------------------------------------- Function for tokenization and lemmatization ignoring punctuation\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         text = f.read()\n",
        "#     doc = nlp(text)  #-------------------------------------------------------- Process the text with spaCy to obtain tokens and lemmas\n",
        "#     lemmatized = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "#     return lemmatized\n",
        "\n",
        "# Only tokenize the corpus\n",
        "def load_tokenized_poem(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    doc = nlp(text)  #---------------------------------------------------------- Process the text with spaCy to obtain tokens\n",
        "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
        "    return tokens\n",
        "\n",
        "# Function to get synonyms [Spanish] using WordNet\n",
        "def get_synonym_es(word):\n",
        "    try:\n",
        "        # Translate the word to English\n",
        "        translator_es_en = GoogleTranslator(source='es', target='en')\n",
        "        word_en = translator_es_en.translate(word).lower()\n",
        "\n",
        "        # Get synonyms in English\n",
        "        synonyms = set()\n",
        "        for synset in wordnet.synsets(word_en):\n",
        "            for lemma in synset.lemmas():\n",
        "                # Avoid adding the same word as a synonym\n",
        "                if lemma.name().lower() != word_en:\n",
        "                    synonyms.add(lemma.name())\n",
        "\n",
        "        # If no synonyms are found, return the original word\n",
        "        if not synonyms:\n",
        "            print(f\"No synonyms found for '{word}'\")\n",
        "            return word\n",
        "\n",
        "        # Select a random synonym\n",
        "        synonym_en = random.choice(list(synonyms))\n",
        "\n",
        "        # Replace underscores with spaces\n",
        "        synonym_en = synonym_en.replace('_', ' ')\n",
        "\n",
        "        # Translate the synonym back to Spanish\n",
        "        translator_en_es = GoogleTranslator(source='en', target='es')\n",
        "        synonym_es = translator_en_es.translate(synonym_en)\n",
        "        print(synonyms)\n",
        "        return synonym_es\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the word: {str(e)}\")\n",
        "        return word\n",
        "\n",
        "# Function to get synonyms [English] using WordNet\n",
        "# def get_synonym_en(word):\n",
        "#     try:\n",
        "#         # Get synonyms in English\n",
        "#         synonyms = set()\n",
        "#         for synset in wordnet.synsets(word_en):\n",
        "#             for lemma in synset.lemmas():\n",
        "#                 # Avoid adding the same word as a synonym\n",
        "#                 if lemma.name().lower() != word_en:\n",
        "#                     synonyms.add(lemma.name())\n",
        "\n",
        "#         # If no synonyms are found, return the original word\n",
        "#         if not synonyms:\n",
        "#             print(f\"No synonyms found for '{word}'\")\n",
        "#             return word\n",
        "\n",
        "#         # Select a random synonym\n",
        "#         synonym_en = random.choice(list(synonyms))\n",
        "\n",
        "#         # Replace underscores with spaces\n",
        "#         synonym_en = synonym_en.replace('_', ' ')\n",
        "\n",
        "#         # Translate the synonym back to Spanish\n",
        "#         print(synonyms)\n",
        "#         return synonym_en\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing the word: {str(e)}\")\n",
        "#         return word\n",
        "\n",
        "\n",
        "# Function to correct grammar using LanguageTool\n",
        "def grammar_corrector(text):\n",
        "    tool = language_tool_python.LanguageTool('es')\n",
        "    return tool.correct(text)\n",
        "\n",
        "# Function to correct the poem using a language model via GPT-4 All\n",
        "def correct_poem(poem): #------------------------------------------------------- Change the promtp to taste depending on the language\n",
        "    correction_prompt = f\"Corrige el siguiente poema (verso) en español, conjugando adecuadamente las palabras lematizadas para darles un sentido coherente y poético, sin alejarte del significado original. Organiza el resultado de manera que fluya como un poema natural, con la estructura y la puntuación adecuada. Devuelve solo el texto corregido dentro de corchetes, sin incluir nada más:\\n\\n{poem}\"\n",
        "\n",
        "    with model.chat_session() as chat:  #--------------------------------------- Initialize the model in chat mode for better response\n",
        "        corrected_poem = chat.generate(correction_prompt)\n",
        "    return corrected_poem.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdzmMyR5SXaO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In this section, the corpus is loaded and processed.\n",
        "This can be any plain text file. The longer the corpus,\n",
        "the better the results the algorithm will produce.\n",
        "Users are encouraged to utilize not only their own poems\n",
        "or texts but also poems, books, and songs they enjoy.\n",
        "Humans learn by imitation, especially in artistic matters.\n",
        "We are all influenced by external factors, references,\n",
        "and more.\n",
        "\"\"\"\n",
        "# Load image from a specified path\n",
        "image_0 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image_1 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799_2.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image_2 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799_3.png\", cv2.IMREAD_GRAYSCALE)\n",
        "image_3 = cv2.imread(\"/content/drive/MyDrive/Stars_Poems/Fuentes/ra7200_dec99799_4.png\", cv2.IMREAD_GRAYSCALE)\n",
        "cv2_imshow(image_0)  #------------------------------------------------------------ Display the image in a window\n",
        "cv2_imshow(image_1)  #------------------------------------------------------------ Display the image in a window\n",
        "cv2_imshow(image_2)  #------------------------------------------------------------ Display the image in a window\n",
        "cv2_imshow(image_3)  #------------------------------------------------------------ Display the image in a window\n",
        "print(\"Image loaded.\")\n",
        "\n",
        "# Load text files for poems from a specified path\n",
        "poems0 = load_poem(\"/Corpus/Primer_internamiento.txt\")\n",
        "poems1 = load_poem(\"/Corpus/relatos.txt\")\n",
        "poems2 = load_poem(\"/Corpus/cartas_.txt\")\n",
        "poems3 = load_poem(\"/Corpus/Garmar_antiguo.txt\")\n",
        "poems4 = load_poem(\"/Corpus/externos.txt\")\n",
        "poems5 = load_poem(\"/Corpus/letras.txt\")\n",
        "# etc. add as many files as you need.\n",
        "\n",
        "# Tokenize text files for poems\n",
        "poems0_tokenized = load_tokenized_poem(\"/Corpus/Primer_internamiento.txt\")\n",
        "poems1_tokenized = load_tokenized_poem(\"/Corpus/relatos.txt\")\n",
        "poems2_tokenized = load_tokenized_poem(\"/Corpus/cartas_.txt\")\n",
        "poems3_tokenized = load_tokenized_poem(\"/Corpus/Garmar_antiguo.txt\")\n",
        "poems4_tokenized = load_tokenized_poem(\"/Corpus/externos.txt\")\n",
        "poems5_tokenized = load_tokenized_poem(\"/Corpus/letras.txt\")\n",
        "#same here, add as many files as you need them :)\n",
        "\n",
        "print(\"Poem files loaded.\")\n",
        "\n",
        "# Combine all lemmatized poems into a single list\n",
        "# Here you can select if the corpus is lemmatized or tokenized by changing the string \"poems\"\n",
        "#poems = poems0 + poems1 + poems2 + poems3 + poems4 + poems5\n",
        "poems = poems0_tokenized + poems1_tokenized + poems2_tokenized + poems3_tokenized + poems4_tokenized + poems5_tokenized\n",
        "print(\"Lemmatized poems unified.\")\n",
        "\n",
        "# Save the result of tokenization and lemmatization to a file with one word per line\n",
        "#This is useful for compare the correct function of the tokenization and lemmatization funtions\n",
        "with open(\"/Outputs/lemmatized_poems.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write('\\n'.join(poems))  #--------------------------------------------- Each word on a new line\n",
        "\n",
        "#with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/tokenized_poems.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "#    file.write('\\n'.join(poems_tokenized))  #----------------------------------- Each word on a new line\n",
        "#print(\"File with lemmatization generated (one word per line).\")\n",
        "\n",
        "with open(\"/Outputs/Corpus.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(' '.join(poems))\n",
        "\n",
        "print(\"File with the final corpus.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcb2HhclTf1K"
      },
      "outputs": [],
      "source": [
        "# Create a Markov chain dictionary avoiding duplicates but maintaining order\n",
        "chain = {}\n",
        "for index, word in enumerate(poems[1:], 1):\n",
        "    key = poems[index - 1]\n",
        "    if key not in chain:\n",
        "        chain[key] = []\n",
        "    if word not in chain[key]:\n",
        "        chain[key].append(word)\n",
        "\n",
        "print(\"Markov chain dictionary created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vHHNPOoT-lV"
      },
      "outputs": [],
      "source": [
        "# Generate a poem\n",
        "word1 = random.choice(list(chain.keys()))\n",
        "message = word1.capitalize()\n",
        "count = 7  #------------------------------------------------------------------- Total number of words in the poem\n",
        "words_per_line = 10  #---------------------------------------------------------- Number of words per line\n",
        "\n",
        "while len(message.split(' ')) < count:\n",
        "\n",
        "    \"\"\"This step is necessary to avoid stationary states;\n",
        "    it is the same gamma-ray distribution with different\n",
        "    color scale treatments and inversions.\"\"\"\n",
        "    selc = random.randint(0, 3)\n",
        "    print()\n",
        "    if selc == 0:\n",
        "        print(\"imagen usada: 1\")\n",
        "        image = image_0\n",
        "    elif selc == 1:\n",
        "        print(\"imagen usada: 2\")\n",
        "        image = image_1\n",
        "    elif selc == 2:\n",
        "        print(\"imagen usada: 3\")\n",
        "        image = image_2\n",
        "    elif selc == 3:\n",
        "        print(\"imagen usada: 4\")\n",
        "        image = image_3\n",
        "\n",
        "\n",
        "    wp = np.ones(len(chain[word1])) / len(chain[word1])\n",
        "    n = len(wp)\n",
        "\n",
        "    # Create Markov matrix from the image\n",
        "    if len(wp) <= image.shape[0]:\n",
        "        markov_n = np.zeros((n, n))\n",
        "        num_sub = image.shape[0] // n\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                sub_mat = image[i*num_sub:(i+1)*num_sub, j*num_sub:(j+1)*num_sub]\n",
        "                markov_n[i, j] = np.mean(sub_mat)\n",
        "    else:\n",
        "        markov_n = cv2.resize(image, (n, n))\n",
        "\n",
        "    # Normalize rows\n",
        "    markov_n = markov_n.astype(np.float64)\n",
        "    markov_n /= markov_n.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Choose the next word based on the matrix\n",
        "    next_w = np.dot(wp, markov_n)\n",
        "    if len(next_w) > 1:\n",
        "        selc_w = np.argmax(next_w)\n",
        "        word2 = chain[word1][selc_w]\n",
        "    else:\n",
        "        word2 = random.choice(list(chain.keys()))\n",
        "\n",
        "\n",
        "    print(next_w[np.argmax(next_w)])\n",
        "    print(\"Current word: \" + word1)\n",
        "    print(\"Number of options: \" + str(len(chain[word1])))\n",
        "    if len(chain[word1]) <= 1:\n",
        "        print(\"No options\")\n",
        "    print(chain[word1])\n",
        "    print(\"Next word: \" + word2)\n",
        "\n",
        "    # Get synonyms if needed\n",
        "    if np.amax(next_w) > 0.8:\n",
        "        word2_2 = get_synonym_es(word2)\n",
        "        #word2_2 = get_synonym_en(word2) #-------------------------------------- If you are using a corpus in English, use this function.\n",
        "                                                                                #Uncomment it in the function declaration section.\n",
        "        #print(\"Condition: yes\")\n",
        "        print(\"Synonym used: \" + word2_2)\n",
        "    else:\n",
        "        word2_2 = word2\n",
        "        #print(\"Condition: no\")\n",
        "\n",
        "    word1 = word2\n",
        "    message += ' ' + word2_2\n",
        "\n",
        "    # Add a line break after a certain number of words\n",
        "    if len(message.split(' ')) % words_per_line == 0:\n",
        "        message = message.rstrip() + '\\n'\n",
        "\n",
        "print(\"Generated poem:\")\n",
        "print(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OmfcShSmUITp"
      },
      "outputs": [],
      "source": [
        "# Apply final grammar correction\n",
        "corrected_message = grammar_corrector(message)\n",
        "\n",
        "# Correct the poem using GPT-4 All\n",
        "output_data = correct_poem(corrected_message)\n",
        "\n",
        "final_message = f\"{message}\\n\\nCorrected version:\\n{output_data}\"\n",
        "\n",
        "# Save results to a specified directory\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/Complete_Results.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(final_message.strip())\n",
        "\n",
        "# Read the file, ignoring any invalid characters\n",
        "with open(\"/content/drive/MyDrive/Stars_Poems/Outputs/Complete_Results.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as output:\n",
        "    print(output.read())\n",
        "\n",
        "print(\"Process completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcAi22C3fQgM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Application of Concrete Poetry\n",
        "\n",
        "**[Optional: The previous result is a fully functional poem, which is the objective of the algorithm. From this point onward, only visual formatting will be applied.]**\n",
        "\n",
        "Depending on the energy value of the last word of the poem, a corresponding function will be selected to create a specific visual shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ1qSWvQjQeh"
      },
      "outputs": [],
      "source": [
        "# Convert text to HTML with random words in italics\n",
        "def text_to_html_random_italics(text, num_italics=1):\n",
        "    words = text.split()\n",
        "    num_italics = min(num_italics, len(words))\n",
        "    random_indices = random.sample(range(len(words)), num_italics)\n",
        "    for idx in random_indices:\n",
        "        words[idx] = f\"<i>{words[idx]}</i>\"\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Shows the text formatted in Colab\n",
        "def display_formatted_text(text):\n",
        "    display(HTML(text))\n",
        "\n",
        "# 1. Diamond\n",
        "def create_diamond(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    max_width = len(words_list) // 2 + 1\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(1, max_width + 1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "    for i in range(max_width - 1, 0, -1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 2. Triangle\n",
        "def create_triangle(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(1, len(words_list) + 1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 3. Inverted triangle\n",
        "def create_reverse_triangle(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(len(words_list), 0, -1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 4. Hourglass\n",
        "def create_hourglass(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(len(words_list), 0, -1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "    for i in range(2, len(words_list) + 1):\n",
        "        line = ' '.join(words_list[:i])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 5. Spiral\n",
        "def create_spiral(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    size = math.ceil(math.sqrt(len(words_list)))\n",
        "    matrix = [['' for _ in range(size)] for _ in range(size)]\n",
        "\n",
        "    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "    x, y = 0, 0\n",
        "    dir_idx = 0\n",
        "    word_idx = 0\n",
        "\n",
        "    while word_idx < len(words_list):\n",
        "        if (0 <= x < size and 0 <= y < size and matrix[x][y] == ''):\n",
        "            matrix[x][y] = words_list[word_idx]\n",
        "            word_idx += 1\n",
        "\n",
        "        next_x = x + directions[dir_idx][0]\n",
        "        next_y = y + directions[dir_idx][1]\n",
        "\n",
        "        if (next_x < 0 or next_x >= size or next_y < 0 or next_y >= size or matrix[next_x][next_y] != ''):\n",
        "            dir_idx = (dir_idx + 1) % 4\n",
        "\n",
        "        x += directions[dir_idx][0]\n",
        "        y += directions[dir_idx][1]\n",
        "\n",
        "    html_output = []\n",
        "    for row in matrix:\n",
        "        line = ' '.join(word for word in row if word)\n",
        "        if line.strip():\n",
        "            html_output.append(line)\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 6. Zigzag\n",
        "def create_zigzag(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces = '&nbsp;' * (4 * (i % 6))\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 7. Circle aprox\n",
        "def create_circle(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    radius = len(words_list) // 4\n",
        "    for i, word in enumerate(words_list):\n",
        "        angle = (i * 2 * math.pi) / len(words_list)\n",
        "        spaces = '&nbsp;' * int(radius + radius * math.cos(angle))\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 8. Wave\n",
        "def create_wave(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces = '&nbsp;' * (10 + int(5 * math.sin(i * 0.5)))\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 9. Cross\n",
        "def create_cross(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    middle = len(words_list) // 2\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(len(words_list)):\n",
        "        if i == middle:\n",
        "            html_output.append(' '.join(words_list))\n",
        "        else:\n",
        "            spaces = '&nbsp;' * (len(words_list[middle]) * 2)\n",
        "            html_output.append(f'{spaces}{words_list[i]}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace; text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 10. Cascade\n",
        "def create_cascade(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces = '&nbsp;' * (i * 4)\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 11. Square\n",
        "def create_square(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    size = math.ceil(math.sqrt(len(words_list)))\n",
        "    html_output = []\n",
        "\n",
        "    for i in range(size):\n",
        "        line_words = words_list[i*size:min((i+1)*size, len(words_list))]\n",
        "        line = ' '.join(line_words)\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 12. Random Scatter\n",
        "def create_random_scatter(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "\n",
        "    for word in words_list:\n",
        "        spaces = '&nbsp;' * random.randint(0, 40)\n",
        "        html_output.append(f'{spaces}{word}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 13. Arrow\n",
        "def create_arrow(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    middle = len(words_list) // 2\n",
        "    html_output = []\n",
        "\n",
        "    # Punta de la flecha\n",
        "    for i in range(middle):\n",
        "        line = ' '.join(words_list[i:i+1])\n",
        "        html_output.append(line.center(50))\n",
        "\n",
        "    # Línea central\n",
        "    html_output.append(' '.join(words_list[middle:]).center(50))\n",
        "\n",
        "    final_html = '<div style=\"text-align: center;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 14. Pyramid\n",
        "def create_pyramid(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "    step_size = 2\n",
        "\n",
        "    for i in range(0, len(words_list), step_size):\n",
        "        line = ' '.join(words_list[i:i+step_size])\n",
        "        spaces = '&nbsp;' * (i * 2)\n",
        "        html_output.append(f'{spaces}{line}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "# 15. Helix\n",
        "def create_helix(words):\n",
        "    formatted_words = text_to_html_random_italics(words)\n",
        "    words_list = formatted_words.split()\n",
        "    html_output = []\n",
        "    amplitude = 20\n",
        "\n",
        "    for i, word in enumerate(words_list):\n",
        "        spaces_left = '&nbsp;' * int(amplitude + amplitude * math.sin(i * 0.5))\n",
        "        spaces_right = '&nbsp;' * int(amplitude + amplitude * math.cos(i * 0.5))\n",
        "        html_output.append(f'{spaces_left}{word}{spaces_right}')\n",
        "\n",
        "    final_html = '<div style=\"font-family: monospace;\">' + '<br>'.join(html_output) + '</div>'\n",
        "    display_formatted_text(final_html)\n",
        "\n",
        "def create_concrete_poetry(text, form, max_value = 15):\n",
        "#form (float or str): Decimal number (0 to max_value) or name of the desired form\n",
        "#max_value (float): Maximum value for the input range (default: 15.0)\n",
        "    available_functions = [\n",
        "        create_diamond,\n",
        "        create_triangle,\n",
        "        create_reverse_triangle,\n",
        "        create_hourglass,\n",
        "        create_spiral,\n",
        "        create_zigzag,\n",
        "        create_circle,\n",
        "        create_wave,\n",
        "        create_cross,\n",
        "        create_cascade,\n",
        "        create_square,\n",
        "        create_random_scatter,\n",
        "        create_arrow,\n",
        "        create_pyramid,\n",
        "        create_helix\n",
        "    ]\n",
        "\n",
        "    #Dictionary for shape names\n",
        "    named_functions = {\n",
        "        'diamond': create_diamond,\n",
        "        'triangle': create_triangle,\n",
        "        'reverse_triangle': create_reverse_triangle,\n",
        "        'hourglass': create_hourglass,\n",
        "        'spiral': create_spiral,\n",
        "        'zigzag': create_zigzag,\n",
        "        'circle': create_circle,\n",
        "        'wave': create_wave,\n",
        "        'cross': create_cross,\n",
        "        'cascade': create_cascade,\n",
        "        'square': create_square,\n",
        "        'random_scatter': create_random_scatter,\n",
        "        'arrow': create_arrow,\n",
        "        'pyramid': create_pyramid,\n",
        "        'helix': create_helix\n",
        "    }\n",
        "\n",
        "    if isinstance(form, (int, float)):\n",
        "        if form < 0 or form > max_value:\n",
        "            form = random.random() * max_value\n",
        "            print(f\"Value out of range. Using random value: {form:.3f}\")\n",
        "        # Map the decimal value to the function index\n",
        "        normalized_value = form / max_value  # Normalize the value between 0 and 1\n",
        "        index = int(normalized_value * len(available_functions))\n",
        "        # Adjust for the case where form == max_value\n",
        "        if index == len(available_functions):\n",
        "            index = len(available_functions) - 1\n",
        "\n",
        "        selected_function = available_functions[index]\n",
        "        print(f\"Value {form:.3f} mapped to shape #{index + 1}\")\n",
        "    elif isinstance(form, str):\n",
        "        if form.lower() not in named_functions:\n",
        "            form = random.random() * max_value\n",
        "            print(f\"Invalid shape name. Using random value: {form:.3f}\")\n",
        "            normalized_value = form / max_value\n",
        "            index = int(normalized_value * len(available_functions))\n",
        "            selected_function = available_functions[index]\n",
        "        else:\n",
        "            selected_function = named_functions[form.lower()]\n",
        "    else:\n",
        "        raise ValueError(\"The 'form' parameter must be a number or a string\")\n",
        "\n",
        "    selected_function(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIA9VUhOpFRH"
      },
      "outputs": [],
      "source": [
        "num = ((np.argmax(next_w)-np.amin(markov_n))*15)/(np.argmax(markov_n) - np.amin(markov_n))\n",
        "print(num)\n",
        "create_concrete_poetry(output_data, form=num, max_value=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlwHwVSKw3Mr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VcAi22C3fQgM"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMt7AQaOVrvXeQesDs6lLbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}